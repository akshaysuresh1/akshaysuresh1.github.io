[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Akshay Suresh, PhD",
    "section": "",
    "text": "Remote-sensing Data Scientist | AI Researcher\n\nHi there! I am Akshay Suresh. Presently, I am a freelance data scientist working on projects involving remote sensing and artificial intelligence (AI) for climate impact. In the past, I built and maintained software pipelines to search terabytes of astronomy data for exotic events, including plausible signs of inteligent life beyond the Earth.\nMajor project highlights:\n\nA LiDAR-based evaluation of rooftop solar feasibility, revealing that 53% of homeowners in DeLand, Florida, can save over $1000 annually by transitioning to solar-powered households.\nDeveloping a scalable tool for near real-time forecast of earthquakes induced from underground carbon storage, thereby enabling proactive measures to minimize the seismic hazard.\nDesigning a novel software to detect repeating radar-like transmissions from potential technologically advanced worlds residing in the Milky Way.\n\nCheck out my About and Projects pages to learn about my educational background and workplace experiences. I also maintain a blog, where I occasionally post about readings that catch my interest. Outside of work, I enjoy following cricket, playing board games, and exploring natural wonders."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nAligning AI with Climate Change Mitigation\n\n\n\n\n\n\n\nPaper Recap\n\n\nAI for Climate\n\n\n\n\nAs AI-powered systems gain increased traction in society, a key concern to address is the climate cost of deploying such powerful models at scale. Depending on the model complexity and the intended applications, AI can accelerate or decrease greenhouse gas (GHG) emissions. Though AI systems accounted for \\(\\leq 1.4\\%\\) of global GHG emissions in 2020, the growing demand for billion-parameter…\n\n\n\n\n\n\nJan 19, 2024\n\n\nAkshay Suresh\n\n\n\n\n\n\n  \n\n\n\n\nScientific Discovery in the Age of Artificial Intelligence\n\n\n\n\n\n\n\nPaper Recap\n\n\nAI for Science\n\n\n\n\nAI techniques, with their power to accelerate scientific discovery, are becoming increasingly integrated into all stages of scientific research, including hypothesis testing, data annotation, system modeling, and insight generation. This paper reviews key scientific breakthroughs from the past decade in the application of AI for science. Here, I touch upon a selection of AI advancements that are…\n\n\n\n\n\n\nJan 7, 2024\n\n\nAkshay Suresh\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "0000-0002-5389-7806\n\n Technical Memo (AI for Climate)\n\nCastiglione, G., Chen, A., Suresh, A., Xiao, H., Kroll, K., Sherman, C., & Weisser, C., Geomechanics for CO\\(_2\\) Sequestration, Frontier Development Lab USA 2022 Technical Results and Findings, pp. 210–226.\n\n\n\n Refereed Journal Articles\nAstronomy: 9 publications during 2016 – 2023 (7 first-author, 2 co-author).\n\nSuresh, A., Gajjar, V., Nagarajan, P., Sheikh, S. Z., et al., (9 authors), A 4–8 GHz Galactic Center Search for Periodic Technosignatures, 2023 AJ 165 255. [Reuters press release]\nSuresh, A., Cordes, J. M., Chatterjee, S., Gajjar, V., et al. (9 authors), 4–8 GHz Fourier-domain Searches for Galactic Center Pulsars, 2022 ApJ 933 121.\nSuresh, A., Cordes, J. M., Chatterjee, S., Gajjar, V., et al. (7 authors), 4–8 GHz Spectrotemporal Emission from the Galactic Center Magnetar PSR J1745—2900, 2021 ApJ 921 101.\nSuresh, A., Chatterjee, S., Cordes, J. M., & Crawford, F., An Arecibo Search for Fast Radio Transients from M87, 2021 ApJ 920 16.\nSuresh, A., Chatterjee, S., Cordes, J. M., Bastian, T. S. & Hallinan, G., Detection of 2–4 GHz Continuum Emission from \\(\\epsilon\\) Eridani, 2020 ApJ 904 138.\nSuresh, A., & Cordes, J. M., Induced Polarization from Birefringent Pulse Splitting in Magnetoionic Media, 2019 ApJ 870 29.\nSuresh, A., Sharma, R., Oberoi, D., et al. (39 authors), Wavelet-based Characterization of Small-scale Solar Emission Features at Low Radio Frequencies, 2017 ApJ 843 19.\nGajjar, V., et al. (22 authors including Suresh, A.), Searching for Broadband Pulsed Beacons from 1883 Stars Using Neural Networks, 2022 ApJ 932 81.\nGajjar, V., et al. (26 authors including Suresh, A.), The Breakthrough Listen Search For Intelligent Life Near the Galactic Center I, 2021 AJ 162 33.\n\n\n\n Peer-reviewed Conference Proceedings\n\nVaillancourt, P. Z., et al. (9 authors including Suresh, A.), Reproducible and Portable Workflows for Scientific Computing and HPC in the Cloud, PEARC 2020.\nOberoi, D., et al. (10 authors including Suresh, A.), Solar Science at Metric Radio Wavelengths: Coming of Age, IAU Symposium 340, 2018.\nSharma, R., Oberoi, D., Suresh, A., & Arjunwardkar, M., Quantifying Weak Non-thermal Meterwave Solar Emission Using Non-imaging Techniques, IAU Symposium 340, 2018.\n\n\n\n Dissertations/Theses\n\nPhD thesis: Suresh, A., Radio Transient Searches from Millisecond to Hour-long Time Scales, Cornell University, USA, 2023.\nMS thesis: Suresh, A., Investigation of Small Scale Weak Solar Emissions at Low Radio Frequencies, IISER Pune, India, 2017."
  },
  {
    "objectID": "boardgames.html",
    "href": "boardgames.html",
    "title": "Board Game Listing (2020 – Present)",
    "section": "",
    "text": "5211\nAnomia\nAzul\nBag of Chips\nBiblios\nCarcassonne\nCatan Junior\nCoup\nDragonwood\nExploding Kittens\nFluxx\nImhotep\nLanterns: The Harvest Festival\nModern Art\nPerudo\nScarabya\nScrabble\nSequence\nSuspend\nTicket to Ride: Europe\nTop that!\nTrails\nTrash Pandas"
  },
  {
    "objectID": "projects/2023_06_01_blipss/index.html#pulse-folding",
    "href": "projects/2023_06_01_blipss/index.html#pulse-folding",
    "title": "Detecting Repeating Radar-like Signals from Alien Worlds",
    "section": "Pulse Folding",
    "text": "Pulse Folding\nConsider the challenge of finding a regularly spaced sequence of pulses buried in noisy data. Ideally, one would like each pulse to be bright and easily discernible above the noise floor. However, that scenario is rare when receiving signals from alien worlds several light years away. Even individual pulses from repeating natural astronomical sources, such as pulsars, are barely distinguishable from noisy fluctuations. How, then, can one detect trains of pulses? Can one exploit the intrinsic regularity of such pulse transmissions to their advantage?\nOne solution would be to break up a noisy time series into finite-length segments and then stack the segments so that individual pulses line up. By lining up and averaging \\(N_{p}\\) single pulses, one builds an average profile with signal-to-ratio elevated by a factor of \\(\\sqrt{N_{p}}\\). An alternate method for periodicity detection would be to use a Fast Fourier Transform. The fast folding algorithm (FFA) belongs to the former category of approaches. A key unknown with such techniques is the segment length or the signal periodicity, i.e., the spacing between the pulses.\n\n\nIllustration of pulse folding. Given a trial folding period \\(P\\), a noisy time series (top panel) of length \\(N\\) is broken into \\(\\lfloor N/P \\rfloor\\) segments (middle panels). These segments are then stacked up. If the chosen trial period \\(P\\) approaches the true period of a repeating signal buried in the data, the stacking operation yields a significant pulse-averaged profile (bottom panel). Figure credit: Ryan S. Lynch, Pulsar Search Collaboratory.\n\nWithout prior knowledge of the true pulse spacing, one’s best bet is to try out numerous trial periods and hope that one of the trials lands close to the actual signal periodicity. Here, the FFA speeds up searches by intelligently selecting trial periods that minimize redundant operations accounting for the discrete sample resolution of an input time series. Once the true signal period has been found, the pulse-averaged signal profile can be constructed. The width of the pulse-averaged profile can then be estimated using a matched filter.\nIn summary, FFA coupled with matched filtering yields estimates of the following two quantities.\n\nPulse spacing or periodicity\nAverage width of the pulse-averaged signal profile\n\n\n\n\n\n\n\nResources on the Fast Folding Algorithm\n\n\n\n\nStaelin, D. H., Fast Folding Algorithm for Detection of Periodic Pulse Trains, 1969 IEEE Proceedings 57 724.\nMorello, V., et al., Optimal Periodicity Searching: Revisiting the Fast Folding Algorithm for Large-scale Pulsar Surveys, 2020 MNRAS 497 4654."
  },
  {
    "objectID": "projects/2023_06_01_blipss/index.html#blipss-workflow",
    "href": "projects/2023_06_01_blipss/index.html#blipss-workflow",
    "title": "Detecting Repeating Radar-like Signals from Alien Worlds",
    "section": "blipss Workflow",
    "text": "blipss Workflow\nBreakthrough Listen is a 10-year initiative (2015 – 2025) to conduct the most comprehensive searches of the Universe for signs of alien technology. Our software blipss, expanded as the “Breakthrough Listen Investigation for Periodic Spectral Signals”, utilizes the FFA routine from riptide to conduct searches for channel-wide repeating signals in radio dynamic spectra.\n\n\n\n\n\n\nDynamic Spectra\n\n\n\nIn astronomy, a spectrum characterizes the variation of light intensity with color. A dynamic spectrum, as the name suggests, captures the time variation of a spectrum. Therefore, dynamic spectra are 2D maps of radiation intensity, with one axis being frequency (or color) and the second axis being time.\nRadio telescopes output discrete samples of their incident radiation intensity at a finite number of radio frequencies. Consequently, dynamic spectra in practice are 2D grids with shape (No. of frequency channels, No. of time samples).\n\n\n\n\nIllustration of the blipss workflow. Consider a dynamic spectrum (left panel) containing in its central channel, a repeating signal with 30 s periodicity. Performing a per-channel FFA, blipss transforms the input dynamic spectrum into a frequency-trial folding period diagram. The repeating signal shows up as clusters of hits at multiples and submultiples of the signal period. The largest signal-to-noise is recorded at the true signal periodicity of 30 s. Image reference: Figure 1 of Suresh et al., 2023."
  },
  {
    "objectID": "projects/2023_06_01_blipss/index.html#surveying-the-galactic-center",
    "href": "projects/2023_06_01_blipss/index.html#surveying-the-galactic-center",
    "title": "Detecting Repeating Radar-like Signals from Alien Worlds",
    "section": "Surveying the Galactic Center",
    "text": "Surveying the Galactic Center\nThe line-of-sight towards the center of the Milky Way offers the highest stellar density of any direction in the sky. By extension from the Drake equation, the Galactic Center then harbors the greatest potential for hosting technologically advanced alien life in the galaxy. Surveying the central 50 light years of the Milky Way with the Green Bank Telescope, we conducted a pilot search for extraterrestrial radar-like transmissions using blipss.\nAs part of our survey, we observed known pulsars to verify the integrity of our data processing. Our data analyses successfully detected all observed pulsars. Searching for channel-wide repeating transmissions with periods between 11–100 seconds and duty cycles (ratio of the average profile width to the pulse period) between 10–50\\(\\%\\), our data revealed no evidence of extraterrestrial signal to within \\(7\\sigma\\) significance. In the process, we constrained the abundance of extraterrestrial transmitters with properties detectable by our analyses to fewer than one in about 600,000 stars at the Galactic Center.\nWe encourage interested readers to read our journal publication for technical details."
  },
  {
    "objectID": "projects/2022_06_15_DSbootcamp/index.html#project-context",
    "href": "projects/2022_06_15_DSbootcamp/index.html#project-context",
    "title": "Predicting Fertilizer Input for Rice Cultivation in India",
    "section": "Project Context",
    "text": "Project Context\nThis project was completed as part of the Erdös Institute Data Science Bootcamp, Spring 2022. Within a hard 2-week deadline, team members were required to define their project goal, identify target stakeholders, gather data, and execute data analyses. The following deliverables were due at the end of the 2-week project window.\n\n1-page executive summary\n5-minutes video presentation\nPresentation slides\nAnnotated Github repository\n\nLinks to our submitted deliverables are provided in a note near the top of this page."
  },
  {
    "objectID": "projects/2022_06_15_DSbootcamp/index.html#project-workflow",
    "href": "projects/2022_06_15_DSbootcamp/index.html#project-workflow",
    "title": "Predicting Fertilizer Input for Rice Cultivation in India",
    "section": "Project Workflow",
    "text": "Project Workflow\n\n\nProject workflow diagram\n\nObjective: Predict mean NPK (nitrogen, phosophorous and potash) fertilizer inputs to achieve specific rice yields in different cultivation environments across India.\nTarget stakeholders: Agriculture policy makers in India\nData sources:\n\nDistrict-level database (1990–2016) maintained by the International Crops Research Institute for the Semi-Arid Tropics (ICRISAT):\n\nRice cultivation data: cropped area, yield, irrigated area\nSeasonality and temperature\nWater cycle data: precipitation, surface runoff, and evapotranspiration\nWind speed\nHistorical NPK fertilizer usage\n\nShapefile of Indian districts from Kaggle"
  },
  {
    "objectID": "projects/2022_06_15_DSbootcamp/index.html#modeling-and-results",
    "href": "projects/2022_06_15_DSbootcamp/index.html#modeling-and-results",
    "title": "Predicting Fertilizer Input for Rice Cultivation in India",
    "section": "Modeling and Results",
    "text": "Modeling and Results\nRice is a hardy crop capable of thriving in a variety of soils, including loams, silts, and gravel. Collating up to 26 years of district-level rice cultivation (cropped area, yield, irrigated area) and environment data (temperature, precipitation, wind speed, evapotranspiration), our analysis involved three steps.\n\nFirstly, we grouped districts with similar ecological parameters into clusters. To do so, we experimented with two unsupervised learning approaches, namely, \\(k\\)-means and hierarchical clustering. Both methods favored the grouping of Indian districts into 6 rice cultivation clusters.\n\n\n\nOptimal clustering of Indian districts based on application of hierarchical clustering to our environmental data (temperature, precipitation, wind speed, evapotranspiration). Note that the above map bears some visual resemblance to the Koppen-Geiger climate classification map of India. However, we caution readers against performing meticulous comparisons between these maps as our algorithms additionally incorporate soil-dependent features such as surface runoff and evapotranspiration.\n\n\nOver independent clusters, we regressed the historical NPK consumption data against rice yield. Here, we trialed simple linear regression, random forest regression, and support vector regression. \n\n\nResults from application of support vector regression to cluster 1 data. Our fit residuals to the potash data (bottom row) look reasonably flat and structure-free. However, the same cannot be said for our nitrogen (top row) and phosphorous (middle row) fit residuals, possibly hinting at either some confounding variable or some parameter cross-correlations unaccounted for in our analyses.\n\n\nThe symmetric mean absolute percent error (SMAPE) offers a convenient difference-based relative measure for comparing model performances across clusters with unequal numbers of data points.\n\n\\[\n{\\rm SMAPE} = \\left( \\frac{100\\%}{N_{\\rm observations}} \\right) \\sum_{\\rm observations} \\left(\\frac{|{\\rm True \\ value} - {\\rm Predicted \\ value}|}{(|{\\rm True \\ value}| + |{\\rm Predicted \\ value}|)/2} \\right)\n\\]\nFor a perfect model (true value = predicted value), \\({\\rm SMAPE} = 0\\%\\). Meanwhile, a null prediction (predicted value = 0) entails \\({\\rm SMAPE} = 200\\%\\).\n\nStudying the above SMAPE barplots, we note that support vector regression marginally outperforms other regression models. However, in general, our regression models are not too accurate, yielding median SMAPE values of about 50% for N and P, and nearly 75% for K."
  },
  {
    "objectID": "projects/2022_06_15_DSbootcamp/index.html#areas-for-improvement",
    "href": "projects/2022_06_15_DSbootcamp/index.html#areas-for-improvement",
    "title": "Predicting Fertilizer Input for Rice Cultivation in India",
    "section": "Areas for Improvement",
    "text": "Areas for Improvement\n\nConsider incorporating data on soil nutrient content, soil type and texture, and solar irradiance to improve the accuracy of model fits.\nAssess the impact of crop rotation and off-season farming practices on the sustainability of a desired crop yield."
  },
  {
    "objectID": "projects/2022_08_31_FDL_CO2/index.html#the-challenge",
    "href": "projects/2022_08_31_FDL_CO2/index.html#the-challenge",
    "title": "Geomechanics for CO\\(_2\\) Sequestration",
    "section": "The Challenge",
    "text": "The Challenge\nAchieving the US goal of carbon neutrality by 2050 will require the storage of over 30 gigatons of CO\\(_2\\) across 30,000 carbon sequestration plants in the country. However, carbon capture and storage, like any other underground fluid injection process, carries the risk of artificially inducing earthquakes. A noteworthy historical example in this regard is Oklahoma, a US state which saw sparse seismic activity during 1974 – 2008. Continued underground wastewater disposal since then has sparked a significant surge in seismic activity across Oklahoma.\n\n\nObserved lagged correlation between wastewater disposal rate (red) and the associated induced seismicity (black) in Oklahoma, USA. Data sources: Oklahoma Geological Survey & Oklahoma Corporation Commission.\n\nForecasts of induced seismicity are essential to permit bulk underground storage of CO\\(_2\\) with minimal seismic hazard. However, existing models of induced seismicity leave much to be desired. Some forecasting models, as evident from the blue, pink, and gray curves in the below figure, are highly inaccurate. Among the more accurate formulations, the physics-based Coupled Coulomb Rate-State (CRS) models are highly site-specific, prohibitively slow, and extremely resource-intensive — requiring supercomputers to train. Further, operation of these models demands significant domain knowledge at the level of a seismology PhD candidate, which limits their utility to a general operator at a CO\\(_2\\) sequestration plant.\n\n\nTime series of cumulative earthquake counts (black) observed in Canterbury, New Zealand between 2010 – 2012. Vertical lines label main quakes with magnitudes exceeding 5.9. Colored curves correspond to various models that have been fit to the data. Shaded areas around colored curves denote Poisson errors on forecasted earthquake counts. Reference: Cattania et al. (2018).\n\nAltogether, equipping CRS models with enhanced speed and accessibility can go a long way towards enabling rapid, accurate induced seismicity forecasts for safe carbon sequestration.\n\n\n\n\n\n\nCoupled Coulomb Rate-State (CRS) model\n\n\n\nWhat does a CRS model take as input? Spatiotemporal grids of subsurface fluid pore pressure and pressurization rate. These grids are computed using knowledge of the spatial distribution of injection wells and time series of fluid injection volume rates at each well.\nWhat does a CRS model output? Time series of cumulative earthquake counts in a specific locality or region of space. Precise locations of earthquake epicenters are not predicted.\n\n\n\n\n\n\n\n\nML-ready data products on Zenodo\n\n\n\n\n\n\n\n\n\n\n\n\n\nLocation (USA)\nArea covered by earthquakes\nNo. of injection wells\nAssumed no. of causally connected earthquake clusters\nCause of induced earthquakes\n\n\n\n\nDecatur, Illinois\n4 km \\(\\times\\) 5 km\n1\n1\nSubsurface CO\\(_2\\) injection\n\n\nCushing, Oklahoma\n4 km \\(\\times\\) 6 km\n1\n1\nUnderground wastewater disposal\n\n\nKansas\n60 km \\(\\times\\) 45 km\n102\n8\nUnderground wastewater disposal\n\n\n\nFor every cluster in each location, we generated separate pressure and earthquake catalogs.\nPressure catalog: epoch, fluid pore pressure at seismicity centroid, pressurization rate at seismicity centroud\nSeismic catalog: epoch, northing, easting, depth, latitude, longitude, magnitude\nWe encourage interested readers to consult our technical report (see note near the top of this page) for references to data sources behind every catalog."
  },
  {
    "objectID": "projects/2022_08_31_FDL_CO2/index.html#the-road-to-a-streamlined-crs-model",
    "href": "projects/2022_08_31_FDL_CO2/index.html#the-road-to-a-streamlined-crs-model",
    "title": "Geomechanics for CO\\(_2\\) Sequestration",
    "section": "The Road to a Streamlined CRS Model",
    "text": "The Road to a Streamlined CRS Model\n\nSpeed gain: We successfully lowered model training time from a whopping 22 hours to a mere 3 minutes on a tablet through the following improvements.\n\n\nImplemented code vectorization in PyTorch for model speedup. Original CRS model was written in Python.\nReduced search space dimensionality by grouping redundant parameters in equations governing the CRS model.\nIntroduced the Adam optimizer from ML to navigate a smooth multi-dimensional loss landscape for model training. Original model used a slow grid search for parameter optimization.\n\n\nAccuracy enhancement: Integrated ML techniques (LSTM, SCINet) with our physics-based CRS model to build reliable forecasts across diverse horizon lengths. \n\n\nLong-range (blue curve in left panel) and short-range forecasts (blue curve in right panel) from our streamlined CRS model with ML enhancements. Red curves in both panels denote the data. The vertical dotted lines in both panels demarcate the boundaries between our training data to the left and our validation data to the right.\n\n\nAccessibility upgrade: Users of our streamlined CRS implementation now only need to understand basic signal processing, some elementary statistics, and the concept of model fitting. Domain knowledge in seismology is no longer a prerequisite for model operation.\n\nQuantifying model gains: \\[\n\\frac{{\\rm No. \\ of \\ CO_2 \\ sequestration \\ sites} \\times {\\rm Model \\  training \\ time} }{\\rm Time \\ between \\ runs} = {\\rm No. \\ of \\ operators \\ required}\n\\]\nOriginal CRS model (not scalable): \\[\n\\frac{{\\rm 30,000 } \\times {\\rm 1 \\ day} }{\\rm 5 \\ days } = {\\rm 6000 \\ analysts \\ from \\ a \\ pool \\ of \\ 10,000 \\ seismiology \\ PhDs}\n\\] This would require restructuring the academic seismological community to do repetitive tasks and stop working on other pressing issues. Also, achieving safety with forecasts would be expensive.\nStreamlined CRS model: \\[\n\\frac{{\\rm 30,000 } \\times {\\rm (1/10) \\ day} }{\\rm 5 \\ days } = {\\rm 600 \\ individuals\\ with \\ basic \\ computing \\ skills \\ (feasible)}\n\\]\nNote:\n\nIn the above calculations, we asummed a gap of 5 days between successive model runs.\nThough our streamlined CRS implementation comes with a training time of only 3 minutes, we assumed a generous modeling time of about 2 hours to account for any hardware setup that may be needed before runs."
  },
  {
    "objectID": "blogposts/2024_01_19_AIigningAIwithClimate/index.html",
    "href": "blogposts/2024_01_19_AIigningAIwithClimate/index.html",
    "title": "Aligning AI with Climate Change Mitigation",
    "section": "",
    "text": "References\n\n\n\n\nKaack, L. H., et al., Aligning Artificial Intelligence with Climate Change Mitigation, 2022 Nature Climate Change 518–527.\nDodge, J., et al., Measuring the Carbon Intensity of AI in Cloud Instances, 2022 ACM Conference on Fairness, Accountability, and Transparency 1877–1894, arXiv:2206.05229.\n\n\n\n\nKaack et al. (2022) proposed a three-category framework for assessing the climate impact of ML.\n\nComputing-related impacts cover GHG emissions arising from electricity consumption for ML computations and embodied emissions associated with computing hardware.\n\nRegarding electricity usage, it is essential to distinguish between the various life-cycle stages involved in ML model production. Model development is the most energy-intensive stage, requiring the identification of a suitable network architecture through trial-and-error training of several models. Once a specific model configuration has been chosen, model training and hyperparameter tuning are next. This stage isn’t as energy-demanding as model development but may need to be performed more frequently in practice.\n\n\nBoxplot showing equivalent carbon intensities from training 11 different AI models in cloud instances. The y-axis (log-scale) measures electricity consumption in MWh translated into equivalent grams of CO\\(_2\\) emissions generated. The x-axis corresponds to various trialed models, including two BERT experiments (finetuning and language model pretraining), partial training of a 6.1 billion-parameter transformer model, three sizes of dense (or fully connected) neural networks, and five sizes of vision transformers. Light blue bars for each model mark the ranges of their respective equivalent emissions, where the top of the bar represents the maximum estimate and the bottom labels the minimum emissions. The heights of the dark blue rectangles contained in the light blue boxes measure interquartile ranges in equivalent emissions. The solid black lines in the dark blue rectangles for various models demarcate their respective average equivalent emissions (or energy consumption).\n\nDodge et al. (2022) measured the electricity consumption from training 11 different models covering natural language processing and computer vision applications. Notably, partial training (up to 13%) of a 6.1 billion-parameter transformer on 256 NVIDIA A100 GPUs for 8 hours used up an astounding 13.8 MWh of energy, i.e., equivalent to the annual electricity requirement of an average US home. Examining the above figure, the equivalent carbon emissions intensity of ML exhibits positive correlations with both the model size and the model complexity. Since foundation models with billions of trainable parameters are energetically expensive to build and train, one way to mitigate climate change is to adapt pretrained, open-source models (when possible) to custom domains via transfer learning or by using parameter-efficient tuning methods.\nAfter model training, we have model inference, i.e., obtaining predictions on test data using a trained model. A single model inference task bears a negligible energy load. However, a model may need to be called thousands of times during deployment, leading to a cumulative energy budget for inference that can exceed those of model development and tuning. For instance, see numbers for Meta in Figure 4 of Wu et al. (2022).\nAs data centers and power systems shift increasingly to using renewable energy sources, emissions arising from ML computations are expected to diminish over time. Embodied emissions from computer infrastructure manufacturing, raw material transport, and end-of-life operations can then acquire increased relevance as a source of GHG emissions. For large data centers, embodied emissions comprise less than 10% of their total GHG emissions. Such centers typically replace servers every 3–4 years to maintain high operational efficiency. A potential downside to keeping new server lifespans short is that it may, in turn, spur growth in server manufacturing demand, thereby contributing to GHG emissions as an offset to gains in operational efficiency. These emissions can be mitigated by reusing old servers and equipment for small to mid-scale ML inference tasks.\n\nImmediate application impacts encompass direct GHG emissions linked to short-term results of ML applications. Specifically, based on the use case, ML may increase or reduce GHG emissions. For instance, computer vision algorithms have helped combat deforestation by enabling enhanced tracking of forest land cover in satellite imagery (Finer et al., 2018). On the contrary, ML-assisted improvements to mining can speed up excavation of oil and natural gas reserves, leading to positive GHG emissions from accelerated fossil fuel exploitation.\nSystem-level impacts refer to potential societal implications that could arise from long-term applications of ML. This category includes both rebound effects and the phenomenon of “lock-in”.\n\nRebound effects can occur when ML increases the efficiency of a service, thereby encouraging growth in the production of the same goods. For instance, ML-enabled autonomous driving can improve fuel efficiency. Still, it may lead to higher personal vehicle ownership rates, which can then increase GHG emissions unless such vehicles are shared or electrified.\nThe lock-in phenomenon is when an ML-enabled product reaches markets first and deters competitors from entering viable market spaces. Returning to our example of autonomous driving, demand for personal autonomous vehicles may result in a drop in private investment in mass public transit systems such as rail and buses, leading to more cars on the road.\nFinally, ML may also influence society by altering the demand for goods and services. For example, a recommender system designed to increase profits for a company may boost the growth of products with embodied GHG emissions. Mandating GHG emissions monitoring and reporting for ML use cases will be essential to shape the design of climate policies governing the development, training, and large-scale deployment of AI models."
  },
  {
    "objectID": "blogposts/2024_01_07_AI4Science/index.html#learning-meaningful-representations-of-scientific-data",
    "href": "blogposts/2024_01_07_AI4Science/index.html#learning-meaningful-representations-of-scientific-data",
    "title": "Scientific Discovery in the Age of Artificial Intelligence",
    "section": "Learning Meaningful Representations of Scientific Data",
    "text": "Learning Meaningful Representations of Scientific Data\nApplications of AI analyses to tackle science problems are often limited by the scarcity of high-quality curated datasets for exploration. Frequently, real-world datasets are incomplete, contain inaccurate observations, and come at variable sample resolutions. Moreover, even in situations where homogeneous data are aplenty, data labeling for supervised learning presents a laborious time investment. Here is where self-supervised learning comes to the rescue.\nSelf-supervised learning (SSL) is a machine learning paradigm that enables a model to automatically generate labels for unstructured data, thereby eliminating the need for large pre-labeled datasets. Building upon a small set of accurate human-annotated data, SSL algorithms learn latent representations of the input data through an iterative procedure. The labels generated in the first iteration are treated as the ground truth in the second iteration and so on.\nGenerative SSL tries to predict masked portions of raw data (text, images, audio, or video) from unmasked segments by learning embeddings of their underlying shared information. On the other hand, contrastive SSL techniques involves defining positive and negative versions of an “anchor” (say, the concept of what a dog looks like). Using a notion of distance in feature space, the algorithm then looks to align positives (e.g., images of dogs) to the anchor while simultaneously repelling the negatives (e.g., images of cats).\n\n\n\n\n\n\nResources on Self-supervised Learning\n\n\n\n\nA Beginner’s Guide to Self-supervised Learning by Rohit Kundu\nFDL 2022 Live Showcase video: Self-Supervised Learning for Change Detection in Synthetic Aperture Radar Data"
  },
  {
    "objectID": "blogposts/2024_01_07_AI4Science/index.html#neural-operators",
    "href": "blogposts/2024_01_07_AI4Science/index.html#neural-operators",
    "title": "Scientific Discovery in the Age of Artificial Intelligence",
    "section": "Neural Operators",
    "text": "Neural Operators\nScientific experiments typically involve discrete measurements of intrinsically continuous quantities. For example, consider wind velocity in flight dynamics or magnetic field strength in tokamak nuclear fusion reactors. Conventional neural networks assume a fixed data discretization and are hence, inflexible at handling raw data sampled at varying resolutions. Neural operators, in contrast, learn mappings between function spaces of the input and output data to allow discretization-invariant predictions. Once these operators are trained, they can be evaluated at any data resolution without a need for model retraining.\n\n\n\n\n\n\nResources on Neural Operators and Their Applications\n\n\n\n\nNeural Operators in PyTorch\nVideo: Anima Anandkumar, Neural operator: A new paradigm for learning PDEs.\nFDL 2021 Live Showcase video: Digital Twin Earth: Coasts.\nKovachki, N., Li, Z., et al., Neural Operator: Learning Maps Between Function Spaces, 2023 JMLR 4(89):1−97."
  },
  {
    "objectID": "blogposts/2024_01_07_AI4Science/index.html#reinforcement-learning-for-navigating-large-hypothesis-spaces",
    "href": "blogposts/2024_01_07_AI4Science/index.html#reinforcement-learning-for-navigating-large-hypothesis-spaces",
    "title": "Scientific Discovery in the Age of Artificial Intelligence",
    "section": "Reinforcement Learning for Navigating Large Hypothesis Spaces",
    "text": "Reinforcement Learning for Navigating Large Hypothesis Spaces\nThe prospect of sampling through all possible competing hypotheses agreeable with the data can seem intimidating at first. Reinforcement learning (RL) instead recasts the problem as an endeavor to find a single good solution that fits the data.\nRL can be thought of as the science and framework of learning to make decisions through interaction. Core concepts of the RL formalism include:\n\nAn environment (governs the dynamics of the problem)\nA scalar reward signal (specifies the goal)\nAn agent comprising necessarily of an agent state and a policy (mapping from agent states to actions).\n\nAt every time step \\(t\\), the agent makes an observation of the environment and perceives a reward (or a penalty). Based on the instantaneous reward, the agent estimates its rewards into the future and decides its next action based on an internal policy. By defining AI policies that maximize rewards in the direction of increased fit between model parameters and the data, RL can drive systems towards optimal solutions. Further, by introducing inductive biases in AI policies, one can incorporate prior scientific knowledge to steer searches towards realistically feasible solutions, thereby enhancing model trust and interpretability.\nNote that the AI policy must allow adequate exploration of the search parameter space so as to not get trapped in a local optimum during the quest to globally maximize the cumulative reward.\n\n\n\n\n\n\nResources on Reinforcement Learning\n\n\n\n\nGoogle DeepMind 2021 Reinforcement Learning Course, University College London\nCoursera: Reinforcement Learning Specialization"
  },
  {
    "objectID": "projects/2023_05_01_agriviz/index.html",
    "href": "projects/2023_05_01_agriviz/index.html",
    "title": "Mapping Historical Crop Yields across India",
    "section": "",
    "text": "This project was completed as part of the Erdös Institute Data Visualization Mini-course, Spring 2023."
  },
  {
    "objectID": "projects/2023_05_01_agriviz/index.html#application-features",
    "href": "projects/2023_05_01_agriviz/index.html#application-features",
    "title": "Mapping Historical Crop Yields across India",
    "section": "Application features",
    "text": "Application features\nI included a live walkthrough of the dashboard in my 2-minutes project presentation video, showcasing the following interactive features.\n\nUser option to zoom in/out of the map\nCursor hover displaying values of the plotted quantity for the selected district.\nDropdown menu for selecting the quantity to visualize. Options include yield (kg/ha), harvested area (1000 ha), and production (1000 tonnes).\nDropdown menu for selecting a crop of interest. Here, the available menu options are Rice, Wheat, Sugarcane, Cotton, and Groundnut.\nSlider to select a year between 1990 and 2017.\n\n\n\nScreenshot of interactive dashboard"
  },
  {
    "objectID": "projects/2023_05_01_agriviz/index.html#project-workflow",
    "href": "projects/2023_05_01_agriviz/index.html#project-workflow",
    "title": "Mapping Historical Crop Yields across India",
    "section": "Project Workflow",
    "text": "Project Workflow"
  },
  {
    "objectID": "projects/2019_12_15_CNNclassifier/index.html#data-prep",
    "href": "projects/2019_12_15_CNNclassifier/index.html#data-prep",
    "title": "Convolutional Neural Networks for Signal Classification in Radio Astronomy",
    "section": "Data Prep",
    "text": "Data Prep\nWe generated simulated data for our study to circumvent the need to manually label thousands of real-world data chunks for supervised learning. As a first pass, we defined the following five classes for our signal classification task, including four categories of interference signals.\n\nllbb: Long-lived broadband interference + background Gaussian noise\nllnb: Long-lived narrowband interference + background Gaussian noise\nslbb: Short-lived broadband interference + background Gaussian noise\nslnb: Short-lived narrowband interference + background Gaussian noise\nnoise: Background Gaussian noise only\n\nFor each of the above classes, we simulated 1200 images of size 128 pixels \\(\\times\\) 128 pixels. From these, we randomly selected and set aside 200 images per category as validation data. The remaining images (1000 per category) formed our training dataset.\n\n\nSample frequency-time images of signals belonging to the llbb (top left), llnb (top right), slbb (bottom left) and slnb (bottom right) output classes.\n\nSince our training data are balanced across all output classes, we adopt the accuracy metric to quantify our model performance. During model training, we observed how the category-integrated model accuracy (defined below) varied with training epoch for our training and validation data. \\[\n{\\rm Network \\ accuracy \\ across \\ all \\ classes} = \\frac{\\rm No.\\  of\\  images \\\ncorrectly \\ classified}{\\rm Total \\ no. \\ of \\ images} \\times 100\\%\n\\] We also investigated confusion matrices built using category-specific model accuracies. We encourage interested readers to consult our project report for relevant details."
  },
  {
    "objectID": "projects/2019_12_15_CNNclassifier/index.html#model-architecture",
    "href": "projects/2019_12_15_CNNclassifier/index.html#model-architecture",
    "title": "Convolutional Neural Networks for Signal Classification in Radio Astronomy",
    "section": "Model Architecture",
    "text": "Model Architecture\nNeural net architectures for various applications are often arrived at through trial and error. In our study, we experimented with toy CNNs of different depths, studying their maximum achieved accuracy when optimally trained.\n\n\n\n\n\n\nNaming convention\n\n\n\nA neural network is said to be an N-layer network if it contains N layers excluding its input layer. A 1-layer network, by definition, then contains zero hidden layers between its input and output layers.\n\n\nOur base network is a 6-layer model whose hidden layers include two convolutional layers, two max-pooling layers, and one fully connected (or dense) layer. We sandwiched ReLU activation functions between our convolutional and max-pooling layers to enable our model to learn non-linear behaviors. Finally, the outputs of our flattened dense layer are normalized to probabilities using a softmax activation function.\n\n\nOur base 6-layer CNN model\n\nWe built models of greater depth by inserting additional convolutional and max-pooling layers ahead of our dense layer. We refer readers to Figures 7–10 of our project report for architecture diagrams of our deeper models."
  },
  {
    "objectID": "projects/2019_12_15_CNNclassifier/index.html#model-training-results",
    "href": "projects/2019_12_15_CNNclassifier/index.html#model-training-results",
    "title": "Convolutional Neural Networks for Signal Classification in Radio Astronomy",
    "section": "Model Training & Results",
    "text": "Model Training & Results\nWe trained all of our models using the categorical cross-entropy (CE) loss function to perform multiclass classification. Further, we uniformly applied the Adam optimizer for model backpropagation with an initial learning rate of \\(10^{-5}\\).\n\n\nAccuracy curves for CNN models of different depths (various colors). Dotted and solid curves represent network performances on the training and validation data respectively.\n\nStarting with our base network (blue curves), we notice that our model performance grows monotonically on the training data (dotted curve) with increasing epoch. However, the network accuracy on the validation data (solid curve) drops after epoch 3, suggesting that the model has now begun to overfit the training data. Based on these trends, we reloaded our saved model weights from epoch 3 to obtain robust predictions using our 6-layer network. Note that our 6-layer model, even at epoch 3, only reaches a maximum accuracy of \\(85\\%\\) on the validation data.\nTraining CNNs of increasing depth, we observe a growth in the maximum network accuracy achieved under conditions of a robust fit. However, the incremental gain in network accuracy diminishes with every added layer. Setting a 95% accuracy threshold, we find that an 8/9-layer CNN model is adequate for our classification problem."
  },
  {
    "objectID": "projects/2019_12_15_CNNclassifier/index.html#areas-for-improvement",
    "href": "projects/2019_12_15_CNNclassifier/index.html#areas-for-improvement",
    "title": "Convolutional Neural Networks for Signal Classification in Radio Astronomy",
    "section": "Areas for Improvement",
    "text": "Areas for Improvement\n\nOur definition of interference signal classes is overly simplistic to allow ready extension to real-world data. Interference signals in radio telescope data often exhibit complex spectrotemporal characteristics that do not fall neatly into any of our predefined output classes. For instance, consider the below sawtooth interference signal (some sort of radar?) seen in data from the Green Bank Telescope in West Viriginia, USA. \nOur models do not account for scenarios where multiple signal classes are present in a single frequency-time snippet. For instance, what if a weak astrophysical signal of interest happens to coincide in time with two bright interference signals of different bandwidths?\n\n\nPerhaps multilabel classification is worth an exercise.\nAlternatively, image segmentation routines may present a path forward."
  },
  {
    "objectID": "projects/2024_05_14_LiDARSolarPotential/index.html#study-location",
    "href": "projects/2024_05_14_LiDARSolarPotential/index.html#study-location",
    "title": "A LiDAR-based Evaluation of Rooftop Solar Potential in DeLand, Florida",
    "section": "Study Location",
    "text": "Study Location\nDeLand is a small residential city in central Florida with an estimated population of 41,263 in 2023. Located approximately 55 km north of Orlando and about 37 km west of Daytona Beach, DeLand offers its residents a rural feel with close access to sprawling business districts. Family establishments comprise over 60% of households in DeLand, with most of them being owner-occupied. On average, residents in DeLand consume about 19.2 MWh of energy per year, and pay approximately $3,612 in annual electricity bills. This expense is roughly 66% higher than the US-averaged annual electricity bill of $2,179 per household. With the immense scope for solar energy in Florida, DeLand’s residential landscape dominated by owner-occupied family homes presents a viable setting for a pilot study to assess house-wise rooftop solar potential."
  },
  {
    "objectID": "projects/2024_05_14_LiDARSolarPotential/index.html#data-sources-and-methodology",
    "href": "projects/2024_05_14_LiDARSolarPotential/index.html#data-sources-and-methodology",
    "title": "A LiDAR-based Evaluation of Rooftop Solar Potential in DeLand, Florida",
    "section": "Data Sources and Methodology",
    "text": "Data Sources and Methodology\n\nRaw data inputs:\n\nUS 3D Elevation Program (3DEP) LiDAR point cloud data (download link) of a 1.52 km \\(\\times\\) 1.52 km area of DeLand, FL\nShapefile of Florida building footprints generated by Microsoft using computer vision models applied to Bing satellite imagery from 2012 and 2019—2020.\n\nMethodology:\nA digital elevation model (DEM) measures the elevation of the bare Earth surface relative to mean sea level. DEMs exclude surface objects such as buildings, vegetation, or infrastructure, from their representations. On the other hand, a digital surface model (DSM) captures the Earth’s surface as seen from overhead imagery. \nI first generated a DEM and a DSM of my study region using my input LiDAR point cloud data. Then, I applied the Area Solar Radiation routine of ArcGIS Pro to compute pixel-level solar insolation estimates in kWh/m\\(^2\\) for the year 2024. Here, the solar insolation of a surface in a year is defined as the cumulative solar energy received per unit area normal to the surface.\nFinally, I derived estimates of the household solar energy production as follows. \\[\n{\\rm Household \\ solar \\ energy \\ production} =  \\epsilon \\Upsilon ({\\rm Footprint \\ Area}) \\times ({\\rm Footprint-averaged \\ solar \\ insolation})\n\\] In the above equation, \\(\\epsilon\\) and \\(\\Upsilon\\) are the solar panel panel efficiency and performance ratio respectively. The solar panel efficiency measures the fraction of incoming solar energy converted into electricity. Meanwhile, the performance ratio quantifies the proportion of generated electrical energy that is preserved through the panel installation. For this study, I chose \\(\\epsilon=22\\%\\) and \\(\\Upsilon=80\\%\\), which are representative values for commercially manufactured solar panels in 2024."
  },
  {
    "objectID": "projects/2024_05_14_LiDARSolarPotential/index.html#results",
    "href": "projects/2024_05_14_LiDARSolarPotential/index.html#results",
    "title": "A LiDAR-based Evaluation of Rooftop Solar Potential in DeLand, Florida",
    "section": "Results",
    "text": "Results\n\nI show below a map of pixel-level solar insolation estimates, revealing shading effects from nearby trees. South-facing rooftops receive the highest insolation, especially evident from pixels with \\(\\gtrsim 1000\\) kWh/m\\(^2\\) insolation. \n\n\n\nPixel-level solar insolation map. Left insets show zoomed-in views of the right panel for two specific localities. The top left inset demonstrates solar insolation values for a single home surrounded by trees. Evidently, solar insolation values are lower at roof bottoms due to shading from adjacent trees. The bottom left inset shows a collection of homes, where shade from tree cover significantly diminishes solar insolation near the edges of various building rooftops.\n\n\nI present a web map showcasing solar energy production estimates for homes in my study region during the year 2024. In the absence of granular knowledge of the annual electricity consumption for each building, I assumed every household to uniformly require the mean energy demand of 19.2 MWh per year.\n757 households (\\(\\approx\\) 76% of homes) in my study region are expected to receive adequate solar exposure to meet the annual average demand of 19.2 MWh per home. Further, production of at least 19.2 MWh of solar energy necessitates a minimum home area of \\(\\approx\\) 108 m\\(^2\\).\n\n\n\nFlorida has an active net metering system that pays residents for selling excess generated solar energy to the state power grid. Assuming a selling rate of 11 cents per kWh, 529 buildings (\\(\\approx\\) 53% of homes) in our study area stand to generate annual profits exceeding $1000, thereby, highlighting the economic potential for widespread rooftop solar adoption in DeLand."
  },
  {
    "objectID": "projects/2024_05_14_LiDARSolarPotential/index.html#areas-for-improvement",
    "href": "projects/2024_05_14_LiDARSolarPotential/index.html#areas-for-improvement",
    "title": "A LiDAR-based Evaluation of Rooftop Solar Potential in DeLand, Florida",
    "section": "Areas for Improvement",
    "text": "Areas for Improvement\n\nEnhance accuracy of pixel-level solar insolation estimates through incorporation of historical climate data and detailed cloud cover assessments using satellite imagery and numerical weather prediction models.\nConsider variable home energy requirements based on building footprint area, occupancy, and appliance usage patterns.\nEvaluate constraints on solar energy production arising from government regulations on installable solar cell capacities for different building sizes."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nA LiDAR-based Evaluation of Rooftop Solar Potential in DeLand, Florida\n\n\n\n\n\n\n\nGeospatial Data Science\n\n\nRemote Sensing\n\n\nSolar Forecasting\n\n\n\n\nThe state of Florida, with access to abundant sunlight, is well positioned to transition from fossil fuel dependency to solar energy. Encouraging residential solar panel installations, particularly in small cities like DeLand, is a crucial aspect of this transition. Light Detection and Ranging (LiDAR) surveys enable detailed maps of residential neighborhoods, thereby permitting homeowners to…\n\n\n\n\n\n\nMay 14, 2024\n\n\nAkshay Suresh\n\n\n\n\n\n\n  \n\n\n\n\nDetecting Repeating Radar-like Signals from Alien Worlds\n\n\n\n\n\n\n\nAstronomy\n\n\nProduction Code Development\n\n\n\n\nThe Search for Extraterrestrial Intelligence (SETI) endeavors to find evidence of advanced alien life in the Universe through signatures of their technologies. Historically, SETI programs have focused mainly on the discovery and follow-up of one-off narrowband (\\(\\sim\\) 1 Hz bandwidth), hard-to-explain events. In contrast, repeating broadband signals analogous to radar offer an…\n\n\n\n\n\n\nJun 1, 2023\n\n\nAkshay Suresh\n\n\n\n\n\n\n  \n\n\n\n\nMapping Historical Crop Yields across India\n\n\n\n\n\n\n\nData Visualization\n\n\n\n\nMaps offer an intuitive means to represent the spatial distribution of data, helping users understand the geographic context and relationships between different locations. With human-induced climate change a reality, understanding evolutionary patterns in nationwide crop yields is essential to ensure food sustainability. India is the world’s second-largest producer of rice, wheat, sugarcane…\n\n\n\n\n\n\nMay 1, 2023\n\n\nAkshay Suresh\n\n\n\n\n\n\n  \n\n\n\n\nGeomechanics for CO\\(_2\\) Sequestration\n\n\n\n\n\n\n\nAI for Climate\n\n\nPhysics-informed AI\n\n\nSupervised Learning\n\n\nTime Series Forecasting\n\n\n\n\nCarbon sequestration involves capturing CO\\(_2\\) from factories or directly from the air and storing it underground as a supercritical fluid. Geologic carbon capture and storage is required to meet global carbon neutrality goals, with over 30 gigatons of CO\\(_2\\) storage needed in the US alone by 2050. However, subsurface carbon storage comes with a catch. One may artificially induce…\n\n\n\n\n\n\nAug 31, 2022\n\n\nAkshay Suresh\n\n\n\n\n\n\n  \n\n\n\n\nPredicting Fertilizer Input for Rice Cultivation in India\n\n\n\n\n\n\n\nAI for Agriculture\n\n\nData Science\n\n\nGeospatial Data Science\n\n\nSupervised Learning\n\n\nUnsupervised Learning\n\n\n\n\nHome to over 1.38 billion people, India is tackling a severe hunger crisis. Though the country has achieved self-sufficiency in grain production, nearly 14% of the population is still undernourished. India’s agricultural landscape is primarily rural, where widespread poverty, low literacy rates, and poor infrastructure lead to questions over its sustainability. Indiscriminate use of fertilizers…\n\n\n\n\n\n\nJun 15, 2022\n\n\nAkshay Suresh\n\n\n\n\n\n\n  \n\n\n\n\nConvolutional Neural Networks for Signal Classification in Radio Astronomy\n\n\n\n\n\n\n\nAstronomy\n\n\nComputer Vision\n\n\nDeep Learning\n\n\nSupervised Learning\n\n\n\n\nRadio waves from human technologies frequently interfere with searches for exotic astrophysical phenomena, yielding hordes of false positives in downstream signal detection pipelines. Discerning astronomical signals of interest from a pile of false positives presents a “needle in a haystack” challenge demanding significant human time investment. Implementing interference blocking at telescope…\n\n\n\n\n\n\nDec 15, 2019\n\n\nAkshay Suresh\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Research Experience\n\n\n\n\n\n01/2024 – Present\n\n\nFreelance Data Scientist, AI for Climate Computer Vision Geospatial Data Remote Sensing\n\n\n\n\n\n08/2017 – 08/2023\n\n\nGraduate Researcher, Cornell University, Ithaca, NY, USA AI and Signal Processing for Automated Astrophysical Event Discovery Deep Learning Signal Processing Python PyTorch Radio Astronomy\n\n\n\n\n\n06/2022 – 08/2022\n\n\nMachine Learning Researcher, Frontier Development Lab USA Time Series Forecast of Earthquake Rates from Underground Carbon Storage Time Series Forecasting LSTM PyTorch Teamwork Climate Adaptation\n\n\n\n\n\n09/2021 – 06/2022\n\n\nVisiting Researcher, University of California, Berkeley, USA Software Development for Radar Detection from Alien Worlds [Reuters press release] [NPR Podcast] Production Code Development Python Search for Extraterrestrial Intelligence\n\n\n\n\n\n\n Technical Skills\n\n\n\n\n\nProgramming\n\n\nPython (PyTorch, scikit-learn, NumPy, pandas, geopandas, xarray, matplotlib, seaborn, plotly), bash scripting, SQL, \\(\\LaTeX\\), HTML\n\n\n\n\n\nMachine learning\n\n\n\n\nSupervised learning:\n\n\nLinear regression, lasso & ridge regularization\n\n\nDecision trees, random forest\n\n\nSupport vector machine\n\n\n\\(K\\)-nearest neighbors\n\n\nConvolutional neural networks\n\n\nRecurrent neural networks\n\n\nLong short-term memory networks\n\n\n\n\nUnsupervised learning:\n\n\n\\(k\\)-means clustering\n\n\nGaussian mixture models\n\n\n\n\nPrincipal component analysis\n\n\nBatch and stochastic gradient descent, Adam optimization\n\n\n\n\n\n\n\nSoftware Engineering\n\n\nProduction code development, high performance computing\n\n\n\n\n\nCloud Computing\n\n\nGoogle Cloud Platform, Amazon Web Services\n\n\n\n\n\nOperating Systems\n\n\nLinux, iOS\n\n\n\n\n\nGeospatial Software\n\n\nArcGIS Pro, QGIS\n\n\n\n\n\nOther Software\n\n\nMicrosoft 365 Suite\n\n\n\n\n\n\n Education\n\n \n \n\n    \n        \n            \n        \n    \n        MS & PhD\n             2017 – 2023 \n        \n        \n            Major: Astronomy \n            Minor: Physics   \n        \n    \n    \n        GPA: 4.0/4.0 \n        Cranson & Edna B. Shelley Outstanding Teaching Assistant Award (2019)  \n    \n    \n\n  \n\n \n    \n        \n            \n              Indian Institute of Science Education & Research, Pune\n        \n    \n        BS & MS with Distinction\n             2012 – 2017 \n        \n        \n            Major: Physics \n            Minor: Mathematics   \n        \n    \n    \n        GPA: 9.9/10 \n        Institute Gold Medal (2017)  \n    \n\n\n\n Licenses & Certifications\n\n\n\n\n\n05/2024\n\n\nAdvanced GIS and Remote Sensing Certification, GIS Vision India\n\n\n\n\n\n04/2023\n\n\nErdös Institute Data Visualization Mini-course\n\n\n\n\n\n06/2022\n\n\nErdös Institute Data Science Bootcamp\n\n\n\n\n\n\n Recreational Activities\n\nI have been an avid cricket fan since my teenage years. When possible, I attempt to experience major cricketing matches live in stadiums worldwide. Attending the World T20 final 2022 at the Melbourne Cricket Ground is my most cherished cricketing memory to date.\n\n\n\n    \n    \n        \n        \n        \n        \n        \n        \n    \n    \n    \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n                        \n    \n    \n    \n    \n        \n        Previous\n    \n    \n    \n    \n        \n        Next\n    \n\n\n\nI enjoy destressing from work through board games and food. Gameistry Entertainment (Chennai, India) and Victory Point Cafe (Berkeley, CA, USA) are two personal favorite board game destinations. I maintain here a list of board games I have played since 2020. Hoping to grow this list further in the coming years!\nI love taking time out from my schedule to appreciate the Earth’s natural wonders. Witnessing the dance of the Aurora Borealis (aka the Northern Lights) in interior Alaska was a dream come true in 2022.\n\n\n\n    \n    \n        \n        \n        \n        \n        \n    \n    \n    \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n                     \n    \n    \n    \n    \n        \n        Previous\n    \n    \n    \n    \n        \n        Next"
  }
]