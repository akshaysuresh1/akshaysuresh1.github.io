[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Akshay Suresh",
    "section": "",
    "text": "AI Researcher | Data Scientist | Freelancer\n\nHi there! I am Akshay Suresh. Professionaly, I love developing artificial intelligence (AI) algorithms to tackle challenging real-world problems and generate sustainable benefit for humanity.\nBuilding on my educational training in physics and data science, I am excited to make impactful contributions in the following domains.\n\nAI for climate change\nAI for healthcare\nEnvironment perception algorithms for autonomous vehicles\nPioneering safe artificial general intelligence\n\nOutside of work, I enjoy following cricket, playing board games, and exploring natural wonders. Visit my About and Projects pages to hear of my experiences in and beyond the workplace."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Research Experience\n\n\n\n\n\n10/2023 – Present\n\n\nIndependent Study & Research Upskilling\n\n\n\n\n\n\n08/2017 – 06/2023\n\n\nGraduate Researcher, Cornell University, Ithaca, NY, USA AI and Signal Processing for Automated Astrophysical Event Discovery Deep Learning Signal Processing Python Pytorch Radio Astronomy\n\n\n\n\n\n06/2022 – 08/2022\n\n\nMachine Learning Researcher, Frontier Development Lab USA Time Series Forecast of Earthquake Rates from Underground Carbon Storage Time Series Forecasting LSTM Pytorch Teamwork Climate Adaptation\n\n\n\n\n\n09/2021 – 06/2022\n\n\nVisiting Researcher, University of California, Berkeley, USA Software Development for Radar Detection from Alien Worlds Production Code Development Python Search for Extraterrestrial Intelligence\n\n\n\n\n\n\n Technical Skills\n\n\n\n\n\nProgramming\n\n\nPython (pytorch, scikit-learn, numpy, pandas, matplotlib, plotly), bash scripting, SQL, \\(\\LaTeX\\), HTML\n\n\n\n\n\nMachine learning\n\n\n\n\nSupervised learning:\n\n\nLinear regression, lasso & ridge regularization\n\n\nDecision trees, random forest\n\n\nSupport vector machine (SVM)\n\n\nk-nearest neighbors (kNN)\n\n\nConvolutional neural networks (CNN)\n\n\nRecurrent neural networks (RNN)\n\n\nLong short-term memory networks (LSTM)\n\n\n\n\nUnsupervised learning:\n\n\nk-means clustering\n\n\nGaussian mixture models (GMM)\n\n\n\n\nPrincipal component analysis (PCA)\n\n\nStochastic gradient descent (SGD)\n\n\nAdam optimization\n\n\n\n\n\n\n\nSoftware Engineering\n\n\nProduction code development, high performance computing (HPC)\n\n\n\n\n\nCloud Computing\n\n\nGoogle Cloud Platform (GCP), Amazon Web Services (AWS)\n\n\n\n\n\nOperating Systems\n\n\nLinux, iOS\n\n\n\n\n\nOther Software\n\n\nMicrosoft 365 Suite\n\n\n\n\n\n\n Education\n\n \n \n\n    \n        \n            \n        \n    \n        MS & PhD\n             2017 – 2023 \n        \n        \n            Major: Astronomy \n            Minor: Physics   \n        \n    \n    \n        GPA: 4.0/4.0 \n        Cranson & Edna B. Shelley Outstanding Teaching Assistant Award (2019)  \n    \n    \n\n  \n\n \n    \n        \n            \n              Indian Institute of Science Education & Research, Pune\n        \n    \n        BS & MS with Distinction\n             2012 – 2017 \n        \n        \n            Major: Physics \n            Minor: Mathematics   \n        \n    \n    \n        GPA: 9.9/10 \n        Institute Gold Medal (2017)  \n    \n\n\n\n Licenses & Certifications\n\n\n\n\n\n04/2023\n\n\nErdös Institute Data Visualization Mini-course\n\n\n\n\n\n06/2022\n\n\nErdös Institute Data Science Bootcamp\n\n\n\n\n\n\n Recreational Activities\n\nI have been an avid cricket fan since my teenage years. When possible, I attempt to experience major cricketing matches live in stadiums worldwide. Attending the World T20 final 2022 at the Melbourne Cricket Ground is most cherished cricketing memory to date.\n\n\n\n    \n    \n        \n        \n        \n        \n        \n        \n    \n    \n    \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n                        \n    \n    \n    \n    \n        \n        Previous\n    \n    \n    \n    \n        \n        Next\n    \n\n\n\nI enjoy destressing from work through board games and food. Gameistry Entertainment (Chennai, India) and Victory Point Cafe (Berkeley, CA, USA) are two personal favorite board game destinations. I maintain here a list of board games I have played since 2020. Hoping to grow this list further in the coming years!\nI love taking time out from my schedule to appreciate the Earth’s natural wonders. Witnessing the dance of the Aurora Borealis (aka the Northern Lights) in interior Alaska was a dream come true in 2022.\n\n\n\n    \n    \n        \n        \n        \n        \n        \n    \n    \n    \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n                     \n    \n    \n    \n    \n        \n        Previous\n    \n    \n    \n    \n        \n        Next"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nConvolutional Neural Networks for Signal Classification in Radio Astronomy\n\n\n\n\n\n\n\nComputer Vision\n\n\nDeep Learning\n\n\nSupervised Learning\n\n\n\n\nThere is an arms race of large language models (LLMs) in industry where companies use different approaches and techniques. Anthropic claims to adopt a more cautious approach that minimizes harm by…\n\n\n\n\n\n\nDec 15, 2019\n\n\nAkshay Suresh\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/2019_12_15_CNNclassifier/index.html#why-this-paper",
    "href": "projects/2019_12_15_CNNclassifier/index.html#why-this-paper",
    "title": "Convolutional Neural Networks for Signal Classification in Radio Astronomy",
    "section": "Why this paper?",
    "text": "Why this paper?\nI’ve been interested in AI safety and responsible AI for several years, and the rise of LLMs has certainly increased stakes. Currently there is an intense arms race among several major tech companies and Anthropic is one of them. They recently published a paper about their LLM and claims to adopt a more cautious approach than others by designing their LLMs to minimize potential harm. They call this constitutional AI (CAI) because their LLMs follow a constitution of principles. I wanted to learn more about how they teach their algorithm to follow these principles."
  },
  {
    "objectID": "projects/2019_12_15_CNNclassifier/index.html#background-knowledge",
    "href": "projects/2019_12_15_CNNclassifier/index.html#background-knowledge",
    "title": "Convolutional Neural Networks for Signal Classification in Radio Astronomy",
    "section": "Background knowledge",
    "text": "Background knowledge\nTo understand this paper properly, it’s better to be familiar with AI alignment problem and reinforcement learning from human feedback (RLHF). AI alignment is about aligning AI systems’ design and values with humanity values such as honesty. Norbert Wiener, an AI researcher back in 1960s, described the AI alignment problem as following:\n\n“If we use, to achieve our purposes, a mechanical agency with whose operation we cannot interfere effectively (…) we had better be quite sure that the purpose put into the machine is the purpose which we really desire.”\n\nRegarding RLHF, I recommend this great summary from Hugging Face. At its core, RLHF is an attempt to distill human feedback into a model (often called reward or preference model) when training LLMs. This is because human feedback is often expensive to collect and difficult to generalize. An important thing to know is that to train this model, practitioners often use ranked preference modeling where human annotators are asked to rank generated text ouptuts from language models. The assumption here is that this approach may mimic human preference of certain responses over others. And because of this preference approach, RLHF papers use Elo scores, a rating system originated from chess to show a player’s winning rates, to evaluate model performance.\nIn terms of the alignment values, Anthropic chose honesty, helpfulness, and harmlessness. The detailed definition of these concepts are described in one of their previous works:\n\n\n\n\n\n\nAskell et al. 2021, What are Helpfulness, Honesty, and Harmlessness?\n\n\n\nHelpful\n\nThe AI should make a clear attempt to perform the task or answer the question posed (as long as it isn’t harmful). It should do this as concisely and efficiently as possible.\nWhen more information is required, the AI should ask relevant follow-up questions and obtain necessary details. It should respond with appropriate levels of sensitivity, insight, and discretion.\nIdeally the AI will also re-direct ill-informed requests, e.g. if asked ‘how can I build a website in assembly language’ it might suggest a different approach.\n\nHonest\n\nAt its most basic level, the AI should give accurate information. Moreover, it should be calibrated (e.g. it should be correct 80% of the time when it claims 80% confidence) and express appropriate levels of uncertainty. It should express its uncertainty without misleading human users.\nCrucially, the AI should be honest about its own capabilities and levels of knowledge – it is not sufficient for it to simply imitate the responses expected from a seemingly humble and honest expert.\nIdeally the AI would also be honest about itself and its own internal state, insofar as that information is available to it.\nHonesty is more objective than helpfulness and harmlessness, so more aspects of honesty training may be possible without human input. This might include calibration training on factual claims and claims about the internal state of the model, and the use of search to augment accuracy.\n\nHarmless\n\nThe AI should not be offensive or discriminatory, either directly or through subtext or bias.\nWhen asked to aid in a dangerous act (e.g. building a bomb), the AI should politely refuse. Ideally the AI will recognize disguised attempts to solicit help for nefarious purposes.\nTo the best of its abilities, the AI should recognize when it may be providing very sensitive or consequential advice and act with appropriate modesty and care.\nWhat behaviors are considered harmful and to what degree will vary across people and cultures. It will also be context-dependent, i.e. it will depend on the nature of the user query, who is using the AI assistant, and the time and place in which the assistant is being used."
  },
  {
    "objectID": "projects/2019_12_15_CNNclassifier/index.html#motivations",
    "href": "projects/2019_12_15_CNNclassifier/index.html#motivations",
    "title": "Convolutional Neural Networks for Signal Classification in Radio Astronomy",
    "section": "Motivations",
    "text": "Motivations\nThe first motivation was scaling supervision. Given that LLMs require numerous examples, it’s better to automate the supervision process and use human annotators to get more curated and high quality answers. This is a similar idea behind the preference modeling in RLHF. The authors called theirs “reinforcement learning from AI Feedback” (RLAIF, not RLHF). A more interesting motivation was building a non-evasive and yet helpful AI assistant. Many currently available AI assistants often simply refuse to answer questions to harmful prompts (e.g., simply saying “I don’t know” or “I can’t answer that”). Their model was never evasive but tried to explain the reasoning behind their negative response to harmful questions. Finally, similar to the first point, they claimed that distilling human supervision into a model could help better understand general aspects of human feedback from many crowd-workers."
  },
  {
    "objectID": "projects/2019_12_15_CNNclassifier/index.html#the-constitutional-ai-approach",
    "href": "projects/2019_12_15_CNNclassifier/index.html#the-constitutional-ai-approach",
    "title": "Convolutional Neural Networks for Signal Classification in Radio Astronomy",
    "section": "The Constitutional AI Approach",
    "text": "The Constitutional AI Approach\n\nSupervised stage\nTheir constitutional AI (CAI) consisted of two stages: a supervised stage and a reinforcement learning stage. In the supervised stage, they used a pretrained LM (“Helpful RLHF model” from their previous work) as a starting point, and red-teamed the model by presenting harmful prompts (by human workers) and sampled the responses. Then, (this is the most interesting part in my opinion!) they used natural language to ask the model to critique and revise its own response based on certain principles. Here’s an example from the paper:\n\n\n\nAppendix A. Sample critiques and revisions. The first two revisions are shown in this screenshot.\n\n\nAppendix C contains a list of principles (constitution) they used to create the critique-revision requests. The paper doesn’t talk much about how they came up with the principles but according to Anthropic’s website, the principles were based on existing documents such as Universal Declaration of Human Rights or Apple’s Terms of Service. As shown in the example above, a response can go through multiple critique-revision requests. The authors found that generally the more revisions mean less harmfulness although the first revision contributes most.\n\n\n\nFig. 5. Preference Model scores of responses and revisions from helpful RLHF models, evaluated on a set of red team prompts.\n\n\n\n\nReinforcement learning (RL) stage\nThe main idea behind this stage is identical to the RL stage in RLHF. The goal is to distill knowledge from a group of responses by training a reward model. The key difference is that these responses are now generated by a model not human (the supervised learning model from the previous stage). The authors called this reward model “feedback model” although it was a bit unclear which exact LMs they were referring to.\nAnother interesting aspect they added here was a “chain-of-thought” approach. This was inspired by Let’s Verify Step by Step, a paper we covered in a previous journal club meeting. Here, after getting a response, the authors added the natural-language phrase “Let’s think step by step” to generate richer intermediate responses from the model.\n\n\n\nAn example from Appendix E.2. Chain-of-Thought Prompts for RL-CAI.\n\n\nIn their final model (Reinforcement Learning-Constitutional AI with Chain of Thought, or RL-CAI w/ CoT), the authors found a major improvement in harmlessness Elo score without compromising the helpfulness Elo score much. Note that in the figure below, Elo score of 0 on the y axis (starting point of the RL model) represents the supervised learning model (SL-CAI), which means the SL-CAI model was used as initial base model for RL.\n\n\n\nFig. 8. Helpfulness (left) and harmlessness (right) Elo scores as a function of the total number of RL training sequences, as judged by crowd-workers via comparison tests.\n\n\nOne interesting aspect of the RL model the authors shared was its behavior when the model was over-fitted. They found that in this case, the response often included boilerplate language such as “you are valid, valued, and cared for.”\n\n\n\nAn example of over-trained RL-CAI model response showing boilerplate language as part of their response (e.g. “you are valid, valued, and cared for”)."
  },
  {
    "objectID": "projects/2019_12_15_CNNclassifier/index.html#broader-impacts",
    "href": "projects/2019_12_15_CNNclassifier/index.html#broader-impacts",
    "title": "Convolutional Neural Networks for Signal Classification in Radio Astronomy",
    "section": "“Broader Impacts”",
    "text": "“Broader Impacts”\nAt the end of the paper, the authors emphasized that natural language feedback could change AI behavior and potentially increase robustness because red-teaming efforts could become more scalable (because feedback supervision can be generated by a model not humans). In section 6.2 Broader Impacts, they briefly mentioned the potential harm of the constitutional AI approach. Using natural language to change AI behavior means it will become much easier to train a malicious AI assistant especially given that this method reduces the need for human feedback."
  },
  {
    "objectID": "projects/2019_12_15_CNNclassifier/index.html#group-discussion",
    "href": "projects/2019_12_15_CNNclassifier/index.html#group-discussion",
    "title": "Convolutional Neural Networks for Signal Classification in Radio Astronomy",
    "section": "Group discussion",
    "text": "Group discussion\nFirst of all, compared to other on-average deep learning papers, we found this paper easier to read. We also appreciated that the authors provided numerous examples. We could see they also tried to do a better job at providing many details of the model although still not enough, especially when they were referring to their previous work.\n\nHarmlessness as an easy alignment\nSome of us suspected that the authors might have chosen harmlessness as their main target of alignment perhaps because it was relatively easier to tackle than other alignment such as helpfulness. The authors did mention the tension between harmfulness and helpfulness in the paper in that an AI assistant could become harmful if it was too eager to be helpful (e.g., providing a detailed answer to a prompt about how to commit a crime). We talked about more nuanced alignments (such as humor) and whether it would be possible to use natural language to change model behavior. Some of us pointed out that harmlessness could be relatively easy because diametrically opposed examples could be easily found in languages.\n\n\nDoes chain-of-thought count as an explanation?\nMany of us were skeptical of treating responses from the chain-of-thought approach as explanations. Most examples shown in the paper seemed reasonable but given that what the model did with a CoT request was nothing more than just generating more detailed responses, we agreed that we should not treat them as step-by-step deductive reasoning. We were interested in looking at CoT examples that might sound gibberish and redundant. I personally also thought this was one of the examples of ML practitioners anthropomorphizing a behavior of an ML model.\n\n\nNo more efforts to understand the model\nMost of us were surprised that the approach of using natural language to critique and revise its own behavior seemed to have worked. Before I read the paper, I was very curious to know what constraints they came up with and how they tried to model complex social concepts such as justice and harm. The fact that their approach seemed to be working was interesting but this also meant that we are in an era where we are no longer trying to change the model behavior at a lower level, but rather we treat the language models as if they are something we don’t completely understand. This paper was completely missing explanations of why this approach actually worked. From my perspective, as other numerous deep learning papers, this paper was saying “we tried this, we don’t exactly know why it works, but it seems to work.”\n\n\nWhy diminish the work and labor of human annotators?\nIn the abstract and at the end of the paper, the authors kept saying their model was trained “without any human labels identifying harmful outputs.” All of us agreed that this was an exaggeration. To train the supervised model, they needed human annotators, and once the supervised model was ready, then they were able to generate AI feedback. Given that Anthropic is a for-profit company that sells AI assistant software, highlighting that the maintenance cost of their system is cheaper than others because human feedback can be replaced by AI feedback, could be a good marketing strategy, but at the cost of marginalizing human labor.\n\n\nHow did you come up with the principles?\nIn Appendix C, the authors provided a comprehensive list of all principles they used to generate critique-revision responses. These were the core principles that guided the model behavior but the authors didn’t mention much about how they curated the list. Some principles were general and others were more specific to particular types of harms such as racism and misogyny. We suspected that there had been an iterative curation process to narrow the list down to these 16 principles specifically. If these were the main drivers of changes in model behavior, we think they should have provided much more details.\n\n\nThe double-edged sword\nFinally, some of us were disappointed that the authors didn’t elaborate much on the potential harm of their approach. They spent a lot of time talking about harmlessness of their algorithms and yet they really fell short when talking about social impacts of their model, especially regarding lowering the barrier for experimenting with LMs and automating supervision by removing human further out of the loop. Particularly for the former, we agreed that it wouldn’t be surprising to see, in near future bad actors take advantage of this approach and come up with a highly toxic, malicious, and harmful AI assistant."
  },
  {
    "objectID": "boardgames.html",
    "href": "boardgames.html",
    "title": "Board Game Listing (2020 – Present)",
    "section": "",
    "text": "Anomia\nAzul\nBiblios\nCarcassonne\nCatan Junior\nCoup\nDragonwood\nExploding Kittens\nFluxx\nImhotep\nLanterns: The Harvest Festival\nModern Art\nPerudo\nScarabya\nScrabble\nSequence\nSuspend\nTicket to Ride: Europe\nTop that!\nTrash Pandas"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Refereed Publications",
    "section": "",
    "text": "0000-0002-5389-7806\n\n Journal articles\nAstronomy: 9 publications during 2016 – 2023 (7 first-author, 2 co-author).\n\n[Reuters press release] Suresh, A., Gajjar, V., Nagarajan, P., Sheikh, S. Z., et al., (9 authors), A 4–8 GHz Galactic Center Search for Periodic Technosignatures, 2023 AJ 165 255.\nSuresh, A., Cordes, J. M., Chatterjee, S., Gajjar, V., et al. (9 authors), 4–8 GHz Fourier-domain Searches for Galactic Center Pulsars, 2022 ApJ 933 121.\nSuresh, A., Cordes, J. M., Chatterjee, S., Gajjar, V., et al. (7 authors), 4–8 GHz Spectrotemporal Emission from the Galactic Center Magnetar PSR J1745—2900, 2021 ApJ 921 101.\nSuresh, A., Chatterjee, S., Cordes, J. M., & Crawford, F., An Arecibo Search for Fast Radio Transients from M87, 2021 ApJ 920 16.\nSuresh, A., Chatterjee, S., Cordes, J. M., Bastian, T. S. & Hallinan, G., Detection of 2–4 GHz Continuum Emission from \\(\\epsilon\\) Eridani, 2020 ApJ 904 138.\nSuresh, A., & Cordes, J. M., Induced Polarization from Birefringent Pulse Splitting in Magnetoionic Media, 2019 ApJ 870 29.\nSuresh, A., Sharma, R., Oberoi, D., et al. (39 authors), Wavelet-based Characterization of Small-scale Solar Emission Features at Low Radio Frequencies, 2017 ApJ 843 19.\nGajjar, V., et al. (22 authors including Suresh, A.), Searching for Broadband Pulsed Beacons from 1883 Stars Using Neural Networks, 2022 ApJ 932 81.\nGajjar, V., et al. (26 authors including Suresh, A.), The Breakthrough Listen Search For Intelligent Life Near the Galactic Center I, 2021 AJ 162 33.\n\n\n\n Conference Proceedings\n\nVaillancourt, P. Z., et al. (9 authors including Suresh, A.), Reproducible and Portable Workflows for Scientific Computing and HPC in the Cloud, PEARC 2020.\nOberoi, D., et al. (10 authors including Suresh, A.), Solar Science at Metric Radio Wavelengths: Coming of Age, IAU Symposium 340, 2018.\nSharma, R., Oberoi, D., Suresh, A., & Arjunwardkar, M., Quantifying Weak Non-thermal Meterwave Solar Emission Using Non-imaging Techniques, IAU Symposium 340, 2018.\n\n\n\n Dissertations\n\nPhD thesis: Suresh, A., Radio Transient Searches from Millisecond to Hour-long Time Scales, Cornell University, USA, 2023.\nMS thesis: Suresh, A., Investigation of Small Scale Weak Solar Emissions at Low Radio Frequencies, IISER Pune, India, 2017."
  }
]